{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502df7b7",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n",
    "Welcome to the Data Cleaning course on Kaggle Learn!\n",
    "\n",
    "Data cleaning is a key part of data science, but it can be deeply frustrating. Why are some of your text fields garbled? What should you do about those missing values? Why arenâ€™t your dates formatted correctly? How can you quickly clean up inconsistent data entry? In this course, you'll learn why you've run into these problems and, more importantly, how to fix them!\n",
    "\n",
    "In this course, youâ€™ll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster. Youâ€™ll work through five hands-on exercises with real, messy data and answer some of your most commonly-asked data cleaning questions.\n",
    "\n",
    "In this notebook, we'll look at how to deal with missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154d1a",
   "metadata": {},
   "source": [
    "## Take A First Look At Data\n",
    "The first thing we'll need to do is load in the libraries and dataset we'll be using.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbed8329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210635/1667417757.py:4: DtypeWarning: Columns (25,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nfl_stats = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/nfl-play-play.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nfl_stats = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/nfl-play-play.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb377e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>down</th>\n",
       "      <th>time</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>TimeSecs</th>\n",
       "      <th>PlayTimeDiff</th>\n",
       "      <th>SideofField</th>\n",
       "      <th>...</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15:00</td>\n",
       "      <td>15</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TEN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.514325</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.060758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14:53</td>\n",
       "      <td>15</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>PIT</td>\n",
       "      <td>...</td>\n",
       "      <td>1.146076</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14:16</td>\n",
       "      <td>15</td>\n",
       "      <td>3556.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>PIT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13:35</td>\n",
       "      <td>14</td>\n",
       "      <td>3515.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>PIT</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.031425</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>-0.049576</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>-0.156239</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13:27</td>\n",
       "      <td>14</td>\n",
       "      <td>3507.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>PIT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>0.441071</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      GameID  Drive  qtr  down   time  TimeUnder  TimeSecs  \\\n",
       "0  2009-09-10  2009091000      1    1   NaN  15:00         15    3600.0   \n",
       "1  2009-09-10  2009091000      1    1   1.0  14:53         15    3593.0   \n",
       "2  2009-09-10  2009091000      1    1   2.0  14:16         15    3556.0   \n",
       "3  2009-09-10  2009091000      1    1   3.0  13:35         14    3515.0   \n",
       "4  2009-09-10  2009091000      1    1   4.0  13:27         14    3507.0   \n",
       "\n",
       "   PlayTimeDiff SideofField  ...    yacEPA  Home_WP_pre  Away_WP_pre  \\\n",
       "0           0.0         TEN  ...       NaN     0.485675     0.514325   \n",
       "1           7.0         PIT  ...  1.146076     0.546433     0.453567   \n",
       "2          37.0         PIT  ...       NaN     0.551088     0.448912   \n",
       "3          41.0         PIT  ... -5.031425     0.510793     0.489207   \n",
       "4           8.0         PIT  ...       NaN     0.461217     0.538783   \n",
       "\n",
       "   Home_WP_post  Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season  \n",
       "0      0.546433      0.453567  0.485675  0.060758       NaN       NaN    2009  \n",
       "1      0.551088      0.448912  0.546433  0.004655 -0.032244  0.036899    2009  \n",
       "2      0.510793      0.489207  0.551088 -0.040295       NaN       NaN    2009  \n",
       "3      0.461217      0.538783  0.510793 -0.049576  0.106663 -0.156239    2009  \n",
       "4      0.558929      0.441071  0.461217  0.097712       NaN       NaN    2009  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e0b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>down</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>TimeSecs</th>\n",
       "      <th>PlayTimeDiff</th>\n",
       "      <th>yrdln</th>\n",
       "      <th>yrdline100</th>\n",
       "      <th>ydstogo</th>\n",
       "      <th>...</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.076880e+05</td>\n",
       "      <td>407688.000000</td>\n",
       "      <td>407688.000000</td>\n",
       "      <td>346534.000000</td>\n",
       "      <td>407688.000000</td>\n",
       "      <td>407464.000000</td>\n",
       "      <td>407244.000000</td>\n",
       "      <td>406848.000000</td>\n",
       "      <td>406848.000000</td>\n",
       "      <td>407688.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>159190.000000</td>\n",
       "      <td>382734.000000</td>\n",
       "      <td>382734.000000</td>\n",
       "      <td>381101.000000</td>\n",
       "      <td>381101.000000</td>\n",
       "      <td>382679.000000</td>\n",
       "      <td>402147.000000</td>\n",
       "      <td>159187.000000</td>\n",
       "      <td>158926.000000</td>\n",
       "      <td>407688.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.013158e+09</td>\n",
       "      <td>12.316158</td>\n",
       "      <td>2.577412</td>\n",
       "      <td>2.002476</td>\n",
       "      <td>7.374200</td>\n",
       "      <td>1695.268944</td>\n",
       "      <td>20.576762</td>\n",
       "      <td>28.488327</td>\n",
       "      <td>48.644081</td>\n",
       "      <td>7.309403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386086</td>\n",
       "      <td>0.534488</td>\n",
       "      <td>0.465965</td>\n",
       "      <td>0.534791</td>\n",
       "      <td>0.465613</td>\n",
       "      <td>0.501320</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.015135</td>\n",
       "      <td>-0.010480</td>\n",
       "      <td>2013.018985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.572839e+06</td>\n",
       "      <td>7.149527</td>\n",
       "      <td>1.129750</td>\n",
       "      <td>1.006353</td>\n",
       "      <td>4.642388</td>\n",
       "      <td>1062.801012</td>\n",
       "      <td>17.969326</td>\n",
       "      <td>12.946471</td>\n",
       "      <td>25.070416</td>\n",
       "      <td>4.869987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.972715</td>\n",
       "      <td>0.285574</td>\n",
       "      <td>0.285629</td>\n",
       "      <td>0.287818</td>\n",
       "      <td>0.287867</td>\n",
       "      <td>0.287445</td>\n",
       "      <td>0.045363</td>\n",
       "      <td>0.056490</td>\n",
       "      <td>0.068139</td>\n",
       "      <td>2.576962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.009091e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997214</td>\n",
       "      <td>-0.999881</td>\n",
       "      <td>-0.986673</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.011101e+09</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>778.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.961115</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>0.231411</td>\n",
       "      <td>0.321701</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.276472</td>\n",
       "      <td>-0.014728</td>\n",
       "      <td>-0.011518</td>\n",
       "      <td>-0.018683</td>\n",
       "      <td>2011.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.013111e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531274</td>\n",
       "      <td>0.469052</td>\n",
       "      <td>0.533609</td>\n",
       "      <td>0.466670</td>\n",
       "      <td>0.504470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.015121e+09</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2585.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485508</td>\n",
       "      <td>0.769232</td>\n",
       "      <td>0.675530</td>\n",
       "      <td>0.772882</td>\n",
       "      <td>0.678833</td>\n",
       "      <td>0.725477</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>0.011431</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.017123e+09</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>943.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.559834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994848</td>\n",
       "      <td>0.994848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GameID          Drive            qtr           down  \\\n",
       "count  4.076880e+05  407688.000000  407688.000000  346534.000000   \n",
       "mean   2.013158e+09      12.316158       2.577412       2.002476   \n",
       "std    2.572839e+06       7.149527       1.129750       1.006353   \n",
       "min    2.009091e+09       1.000000       1.000000       1.000000   \n",
       "25%    2.011101e+09       6.000000       2.000000       1.000000   \n",
       "50%    2.013111e+09      12.000000       3.000000       2.000000   \n",
       "75%    2.015121e+09      18.000000       4.000000       3.000000   \n",
       "max    2.017123e+09      35.000000       5.000000       4.000000   \n",
       "\n",
       "           TimeUnder       TimeSecs   PlayTimeDiff          yrdln  \\\n",
       "count  407688.000000  407464.000000  407244.000000  406848.000000   \n",
       "mean        7.374200    1695.268944      20.576762      28.488327   \n",
       "std         4.642388    1062.801012      17.969326      12.946471   \n",
       "min         0.000000    -900.000000       0.000000       1.000000   \n",
       "25%         3.000000     778.000000       5.000000      20.000000   \n",
       "50%         7.000000    1800.000000      17.000000      30.000000   \n",
       "75%        11.000000    2585.000000      37.000000      39.000000   \n",
       "max        15.000000    3600.000000     943.000000      50.000000   \n",
       "\n",
       "          yrdline100        ydstogo  ...         yacEPA    Home_WP_pre  \\\n",
       "count  406848.000000  407688.000000  ...  159190.000000  382734.000000   \n",
       "mean       48.644081       7.309403  ...      -0.386086       0.534488   \n",
       "std        25.070416       4.869987  ...       1.972715       0.285574   \n",
       "min         1.000000       0.000000  ...     -14.000000       0.000000   \n",
       "25%        30.000000       3.000000  ...      -0.961115       0.325123   \n",
       "50%        49.000000       9.000000  ...       0.000000       0.531274   \n",
       "75%        70.000000      10.000000  ...       0.485508       0.769232   \n",
       "max        99.000000      50.000000  ...       9.559834       1.000000   \n",
       "\n",
       "         Away_WP_pre   Home_WP_post   Away_WP_post       Win_Prob  \\\n",
       "count  382734.000000  381101.000000  381101.000000  382679.000000   \n",
       "mean        0.465965       0.534791       0.465613       0.501320   \n",
       "std         0.285629       0.287818       0.287867       0.287445   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.231411       0.321701       0.227694       0.276472   \n",
       "50%         0.469052       0.533609       0.466670       0.504470   \n",
       "75%         0.675530       0.772882       0.678833       0.725477   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 WPA         airWPA         yacWPA         Season  \n",
       "count  402147.000000  159187.000000  158926.000000  407688.000000  \n",
       "mean        0.002099       0.015135      -0.010480    2013.018985  \n",
       "std         0.045363       0.056490       0.068139       2.576962  \n",
       "min        -0.997214      -0.999881      -0.986673    2009.000000  \n",
       "25%        -0.014728      -0.011518      -0.018683    2011.000000  \n",
       "50%         0.000000       0.003441       0.000000    2013.000000  \n",
       "75%         0.014684       0.035792       0.011431    2015.000000  \n",
       "max         0.994848       0.994848       1.000000    2017.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1512b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754a420",
   "metadata": {},
   "source": [
    "## How many missing data points do we have?\n",
    "\n",
    "Ok, now we know that we do have some missing values. Let's see how many we have in each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab5ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "GameID           0\n",
       "Drive            0\n",
       "qtr              0\n",
       "down         61154\n",
       "             ...  \n",
       "Win_Prob     25009\n",
       "WPA           5541\n",
       "airWPA      248501\n",
       "yacWPA      248762\n",
       "Season           0\n",
       "Length: 102, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the total missing values in each column\n",
    "missing_values_count = nfl_stats.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a999e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                0\n",
       "GameID              0\n",
       "Drive               0\n",
       "qtr                 0\n",
       "down            61154\n",
       "time              224\n",
       "TimeUnder           0\n",
       "TimeSecs          224\n",
       "PlayTimeDiff      444\n",
       "SideofField       528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing values in only the first 10 columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9909c3c",
   "metadata": {},
   "source": [
    "That seems like a lot! It might be helpful to see what percentage of the values in our dataset were missing to give us a better sense of the scale of this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b1a8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407688, 102)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b9a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.66722370547874\n"
     ]
    }
   ],
   "source": [
    "total_cells = np.product(nfl_stats.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "#percent of missing data\n",
    "percent_missing = (total_missing/total_cells)*100\n",
    "print(percent_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701323a1",
   "metadata": {},
   "source": [
    " Wow, almost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.\n",
    " \n",
    " ## Figure out why the data is missing\n",
    " This is the point at which we get into the part of data science that I like to call \"data intution\", by which I mean \"really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis\". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important questions you can ask yourself to help figure this out is this:\n",
    "\n",
    "### Is this value missing because it wasn't recorded or because it doesn't exist?\n",
    "\n",
    "If a value is missing becuase it doesn't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probably do want to keep as NaN. On the other hand, if a value is missing because it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. This is called imputation, and we'll learn how to do it next! :)\n",
    "\n",
    "Let's work through an example. Looking at the number of missing values in the nfl_data dataframe, I notice that the column \"TimesSec\" has a lot of missing values in it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e05f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                0\n",
       "GameID              0\n",
       "Drive               0\n",
       "qtr                 0\n",
       "down            61154\n",
       "time              224\n",
       "TimeUnder           0\n",
       "TimeSecs          224\n",
       "PlayTimeDiff      444\n",
       "SideofField       528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57050685",
   "metadata": {},
   "source": [
    "By looking at the documentation[https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016], I can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.\n",
    "\n",
    "On the other hand, there are other fields, like \"PenalizedTeam\" that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say which team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like \"neither\" and use that to replace the NA's\n",
    "\n",
    "Tip: This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.\n",
    "\n",
    "If you're doing very careful data analysis, this is the point at which you'd look at each column individually to figure out the best strategy for filling those missing values. For the rest of this notebook, we'll cover some \"quick and dirty\" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data\n",
    "\n",
    "## Drop Missing Values\n",
    "If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don't generally recommend this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)\n",
    "\n",
    "If you're sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this. Let's try it out on our NFL dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717b0a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>down</th>\n",
       "      <th>time</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>TimeSecs</th>\n",
       "      <th>PlayTimeDiff</th>\n",
       "      <th>SideofField</th>\n",
       "      <th>...</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, GameID, Drive, qtr, down, time, TimeUnder, TimeSecs, PlayTimeDiff, SideofField, yrdln, yrdline100, ydstogo, ydsnet, GoalToGo, FirstDown, posteam, DefensiveTeam, desc, PlayAttempted, Yards.Gained, sp, Touchdown, ExPointResult, TwoPointConv, DefTwoPoint, Safety, Onsidekick, PuntResult, PlayType, Passer, Passer_ID, PassAttempt, PassOutcome, PassLength, AirYards, YardsAfterCatch, QBHit, PassLocation, InterceptionThrown, Interceptor, Rusher, Rusher_ID, RushAttempt, RunLocation, RunGap, Receiver, Receiver_ID, Reception, ReturnResult, Returner, BlockingPlayer, Tackler1, Tackler2, FieldGoalResult, FieldGoalDistance, Fumble, RecFumbTeam, RecFumbPlayer, Sack, Challenge.Replay, ChalReplayResult, Accepted.Penalty, PenalizedTeam, PenaltyType, PenalizedPlayer, Penalty.Yards, PosTeamScore, DefTeamScore, ScoreDiff, AbsScoreDiff, HomeTeam, AwayTeam, Timeout_Indicator, Timeout_Team, posteam_timeouts_pre, HomeTimeouts_Remaining_Pre, AwayTimeouts_Remaining_Pre, HomeTimeouts_Remaining_Post, AwayTimeouts_Remaining_Post, No_Score_Prob, Opp_Field_Goal_Prob, Opp_Safety_Prob, Opp_Touchdown_Prob, Field_Goal_Prob, Safety_Prob, Touchdown_Prob, ExPoint_Prob, TwoPoint_Prob, ExpPts, EPA, airEPA, yacEPA, Home_WP_pre, Away_WP_pre, Home_WP_post, Away_WP_post, Win_Prob, WPA, airWPA, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 102 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_stats.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37345f5b",
   "metadata": {},
   "source": [
    "Oh dear, it looks like that's removed all our data! ðŸ˜± This is because every row in our dataset had at least one missing value. We might have better luck removing all the columns that have at least one missing value instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf180108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>ydstogo</th>\n",
       "      <th>ydsnet</th>\n",
       "      <th>PlayAttempted</th>\n",
       "      <th>Yards.Gained</th>\n",
       "      <th>sp</th>\n",
       "      <th>...</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>Timeout_Indicator</th>\n",
       "      <th>posteam_timeouts_pre</th>\n",
       "      <th>HomeTimeouts_Remaining_Pre</th>\n",
       "      <th>AwayTimeouts_Remaining_Pre</th>\n",
       "      <th>HomeTimeouts_Remaining_Post</th>\n",
       "      <th>AwayTimeouts_Remaining_Post</th>\n",
       "      <th>ExPoint_Prob</th>\n",
       "      <th>TwoPoint_Prob</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>TEN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>TEN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>TEN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>TEN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-09-10</td>\n",
       "      <td>2009091000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>TEN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      GameID  Drive  qtr  TimeUnder  ydstogo  ydsnet  \\\n",
       "0  2009-09-10  2009091000      1    1         15        0       0   \n",
       "1  2009-09-10  2009091000      1    1         15       10       5   \n",
       "2  2009-09-10  2009091000      1    1         15        5       2   \n",
       "3  2009-09-10  2009091000      1    1         14        8       2   \n",
       "4  2009-09-10  2009091000      1    1         14        8       2   \n",
       "\n",
       "   PlayAttempted  Yards.Gained  sp  ...  AwayTeam  Timeout_Indicator  \\\n",
       "0              1            39   0  ...       TEN                  0   \n",
       "1              1             5   0  ...       TEN                  0   \n",
       "2              1            -3   0  ...       TEN                  0   \n",
       "3              1             0   0  ...       TEN                  0   \n",
       "4              1             0   0  ...       TEN                  0   \n",
       "\n",
       "   posteam_timeouts_pre HomeTimeouts_Remaining_Pre  \\\n",
       "0                     3                          3   \n",
       "1                     3                          3   \n",
       "2                     3                          3   \n",
       "3                     3                          3   \n",
       "4                     3                          3   \n",
       "\n",
       "   AwayTimeouts_Remaining_Pre  HomeTimeouts_Remaining_Post  \\\n",
       "0                           3                            3   \n",
       "1                           3                            3   \n",
       "2                           3                            3   \n",
       "3                           3                            3   \n",
       "4                           3                            3   \n",
       "\n",
       "   AwayTimeouts_Remaining_Post  ExPoint_Prob  TwoPoint_Prob  Season  \n",
       "0                            3           0.0            0.0    2009  \n",
       "1                            3           0.0            0.0    2009  \n",
       "2                            3           0.0            0.0    2009  \n",
       "3                            3           0.0            0.0    2009  \n",
       "4                            3           0.0            0.0    2009  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove all columns with at least one missing value\n",
    "columns_with_na_dropped = nfl_stats.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996c422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns with data: 102 \n",
      "\n",
      "Dataset with na's dropped: 37\n"
     ]
    }
   ],
   "source": [
    "#just how much columns with data did we lose?\n",
    "print(\"Original columns with data: %d \\n\" % nfl_stats.shape[1])\n",
    "print(\"Dataset with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd4599",
   "metadata": {},
   "source": [
    "We've lost quite a bit of data, but at this point we have successfully removed all the NaN's from our data.\n",
    "\n",
    "## Filling In Missing Values Automatically\n",
    "\n",
    "Another option is to try and fill in the missing values. For this next bit, I'm getting a small sub-section of the NFL data so that it will print well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61320870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>airEPA</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.014474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.514325</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.060758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077907</td>\n",
       "      <td>-1.068169</td>\n",
       "      <td>1.146076</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.402760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.712583</td>\n",
       "      <td>3.318841</td>\n",
       "      <td>-5.031425</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>-0.049576</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>-0.156239</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.097796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>0.441071</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EPA    airEPA    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  \\\n",
       "0  2.014474       NaN       NaN     0.485675     0.514325      0.546433   \n",
       "1  0.077907 -1.068169  1.146076     0.546433     0.453567      0.551088   \n",
       "2 -1.402760       NaN       NaN     0.551088     0.448912      0.510793   \n",
       "3 -1.712583  3.318841 -5.031425     0.510793     0.489207      0.461217   \n",
       "4  2.097796       NaN       NaN     0.461217     0.538783      0.558929   \n",
       "\n",
       "   Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season  \n",
       "0      0.453567  0.485675  0.060758       NaN       NaN    2009  \n",
       "1      0.448912  0.546433  0.004655 -0.032244  0.036899    2009  \n",
       "2      0.489207  0.551088 -0.040295       NaN       NaN    2009  \n",
       "3      0.538783  0.510793 -0.049576  0.106663 -0.156239    2009  \n",
       "4      0.441071  0.461217  0.097712       NaN       NaN    2009  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a subset of the nfl data\n",
    "subset_of_nfl_data = nfl_stats.loc[:, \"EPA\":\"Season\"].head()\n",
    "subset_of_nfl_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9060a0e",
   "metadata": {},
   "source": [
    "We can use the Panda's fillna() function to fill in missing values in a dataframe for us. One option we have is to specify what we want the NaN values to be replaced with. Here, I'm saying that I would like to replace all the NaN values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b2d65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>airEPA</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.014474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.514325</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.060758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077907</td>\n",
       "      <td>-1.068169</td>\n",
       "      <td>1.146076</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.402760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.712583</td>\n",
       "      <td>3.318841</td>\n",
       "      <td>-5.031425</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>-0.049576</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>-0.156239</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.097796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>0.441071</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EPA    airEPA    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  \\\n",
       "0  2.014474  0.000000  0.000000     0.485675     0.514325      0.546433   \n",
       "1  0.077907 -1.068169  1.146076     0.546433     0.453567      0.551088   \n",
       "2 -1.402760  0.000000  0.000000     0.551088     0.448912      0.510793   \n",
       "3 -1.712583  3.318841 -5.031425     0.510793     0.489207      0.461217   \n",
       "4  2.097796  0.000000  0.000000     0.461217     0.538783      0.558929   \n",
       "\n",
       "   Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season  \n",
       "0      0.453567  0.485675  0.060758  0.000000  0.000000    2009  \n",
       "1      0.448912  0.546433  0.004655 -0.032244  0.036899    2009  \n",
       "2      0.489207  0.551088 -0.040295  0.000000  0.000000    2009  \n",
       "3      0.538783  0.510793 -0.049576  0.106663 -0.156239    2009  \n",
       "4      0.441071  0.461217  0.097712  0.000000  0.000000    2009  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_of_nfl_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1159d",
   "metadata": {},
   "source": [
    "I could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925c8d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>airEPA</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.014474</td>\n",
       "      <td>-1.068169</td>\n",
       "      <td>1.146076</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.514325</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.060758</td>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077907</td>\n",
       "      <td>-1.068169</td>\n",
       "      <td>1.146076</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.453567</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>-0.032244</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.402760</td>\n",
       "      <td>3.318841</td>\n",
       "      <td>-5.031425</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.551088</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>-0.156239</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.712583</td>\n",
       "      <td>3.318841</td>\n",
       "      <td>-5.031425</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.489207</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>-0.049576</td>\n",
       "      <td>0.106663</td>\n",
       "      <td>-0.156239</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.097796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.538783</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>0.441071</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EPA    airEPA    yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  \\\n",
       "0  2.014474 -1.068169  1.146076     0.485675     0.514325      0.546433   \n",
       "1  0.077907 -1.068169  1.146076     0.546433     0.453567      0.551088   \n",
       "2 -1.402760  3.318841 -5.031425     0.551088     0.448912      0.510793   \n",
       "3 -1.712583  3.318841 -5.031425     0.510793     0.489207      0.461217   \n",
       "4  2.097796  0.000000  0.000000     0.461217     0.538783      0.558929   \n",
       "\n",
       "   Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season  \n",
       "0      0.453567  0.485675  0.060758 -0.032244  0.036899    2009  \n",
       "1      0.448912  0.546433  0.004655 -0.032244  0.036899    2009  \n",
       "2      0.489207  0.551088 -0.040295  0.106663 -0.156239    2009  \n",
       "3      0.538783  0.510793 -0.049576  0.106663 -0.156239    2009  \n",
       "4      0.441071  0.461217  0.097712  0.000000  0.000000    2009  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_of_nfl_data.fillna(method='bfill',axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91f7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81add90f",
   "metadata": {},
   "source": [
    "# Scaling and Normalisation\n",
    "The first thing we'll need to do is load in the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248a34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box Cox transformation\n",
    "from scipy import stats\n",
    "\n",
    "#for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "#plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3c3a9",
   "metadata": {},
   "source": [
    "## Scaling vs. Normalization: What's the difference?\n",
    "One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\n",
    "\n",
    "in scaling, you're changing the range of your data, while\n",
    "in normalization, you're changing the shape of the distribution of your data.\n",
    "Let's talk a little more in-depth about each of these options.\n",
    "\n",
    "### SCALING\n",
    "\n",
    "This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points are, like support vector machines (SVM)[https://en.wikipedia.org/wiki/Support_vector_machine] or k-nearest neighbors (KNN)[]. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n",
    "\n",
    "For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n",
    "\n",
    "By scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in the following exercise!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82171fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1000 random data points\n",
    "original_data = np.random.exponential(size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee5a3643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a13f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min-max scaling the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data,columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26d5d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt both together to compare\n",
    "original_data = pd.DataFrame(np.random.exponential(size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b3fbbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_210635/1643487477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# plot both together to compare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mhistplot\u001b[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munivariate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         p.plot_univariate_histogram(\n\u001b[0m\u001b[1;32m   1433\u001b[0m             \u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0melement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mplot_univariate_histogram\u001b[0;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mline_kws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 line, = ax.plot(\n\u001b[0m\u001b[1;32m    656\u001b[0m                     \u001b[0;34m*\u001b[0m\u001b[0mline_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mline_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5196\u001b[0m         \u001b[0;31m# Because we ruled out integer above, we always get an arraylike here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5198\u001b[0;31m             \u001b[0mdisallow_ndim_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5200\u001b[0m         \u001b[0;31m# NB: Using _constructor._simple_new would break if MultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mdisallow_ndim_indexing\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \"\"\"\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0;34m\"Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;34m\"supported. Convert to a numpy array before indexing instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "original_data = pd.DataFrame(np.random.exponential(size=1000))\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,3))\n",
    "original_data = np.random.exponential(size=1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34179539",
   "metadata": {},
   "source": [
    "Notice that the shape of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26850881",
   "metadata": {},
   "source": [
    "Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "Normal distribution: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "In general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "The method we're using to normalize here is called the Box-Cox Transformation. Let's take a quick peek at what normalizing some data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8199826",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = stats.boxcox(original_data)\n",
    "\n",
    "fig, ax=plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d24c42",
   "metadata": {},
   "source": [
    "## Parsing Dates\n",
    "The first thing we'll need to do is load in the libraries and dataset we'll be using. We'll be working with a dataset that contains information on landslides that occured between 2007 and 2016. In the following exercise, you'll apply your new skills to a dataset of worldwide earthquakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "landslides = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/catalog.csv')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c92794",
   "metadata": {},
   "source": [
    "Check the data type of our date column\n",
    "We begin by taking a look at the first five rows of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8a7a0",
   "metadata": {},
   "source": [
    "We'll be working with the \"date\" column from the landslides dataframe. Let's make sure it actually looks like it contains dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(landslides['date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45237b7a",
   "metadata": {},
   "source": [
    "Yep, those are dates! But just because I, a human, can tell that these are dates doesn't mean that Python knows that they're dates. Notice that at the bottom of the output of head(), you can see that it says that the data type of this column is \"object\".\n",
    "\n",
    "Pandas uses the \"object\" dtype for storing various types of data types, but most often when you see a column with the dtype \"object\" it will have strings in it.\n",
    "\n",
    "If you check the pandas dtype documentation here, you'll notice that there's also a specific datetime64 dtypes. Because the dtype of our column is object rather than datetime64, we can tell that Python doesn't know that this column contains dates.\n",
    "\n",
    "We can also look at just the dtype of a column without printing the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b2b7e",
   "metadata": {},
   "source": [
    "You may have to check the numpy documentation to match the letter code to the dtype of the object. \"O\" is the code for \"object\", so we can see that these two methods give us the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051479f4",
   "metadata": {},
   "source": [
    "## Convert Our Date Column To DateTime\n",
    "\n",
    "Now that we know that our date column isn't being recognized as a date, it's time to convert it so that it is recognized as a date. This is called \"parsing dates\" because we're taking in a string and identifying its component parts.\n",
    "\n",
    "We can determine what the format of our dates are with a guide called \"strftime directive\", which you can find more information on at this link. The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date, but the most common are %d for day, %m for month, %y for a two-digit year and %Y for a four digit year.\n",
    "Some examples:\n",
    "\n",
    "1/17/07 has the format \"%m/%d/%y\"\n",
    "17-1-2007 has the format \"%d-%m-%Y\"\n",
    "Looking back up at the head of the \"date\" column in the landslides dataset, we can see that it's in the format \"month/day/two-digit year\", so we can use the same syntax as the first example to parse in our dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column to hold the coversion\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e5eaf",
   "metadata": {},
   "source": [
    "Now when I check the first few rows of the new column, I can see that the dtype is datetime64. I can also see that my dates have been slightly rearranged so that they fit the default order datetime objects (year-month-day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides['date_parsed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8ba80",
   "metadata": {},
   "source": [
    "Now that our dates are parsed correctly, we can interact with them in useful ways.\n",
    "\n",
    "What if I run into an error with multiple date formats? While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you can have pandas try to infer what the right date format should be. You can do that like so:\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)\n",
    "\n",
    "Why don't you always use infer_datetime_format = True? There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates.\n",
    "\n",
    "### Selecting Day of the month\n",
    "Now that we have a column of parsed dates, we can extract information like the day of the month that a landslide occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99affd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_ofmonth_landslides = landslides['date_parsed'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_ofmonth_landslides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d755827",
   "metadata": {},
   "source": [
    "f we tried to get the same information from the original \"date\" column, we would get an error: AttributeError: Can only use .dt accessor with datetimelike values. This is because dt.day doesn't know how to deal with a column with the dtype \"object\". Even though our dataframe has dates in it, we have to parse them before we can interact with them in a useful way.\n",
    "\n",
    "### Plot The Date Of The Month To Check Parsing\n",
    "One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense.\n",
    "\n",
    "To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.) Let's see if that's the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11658090",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_ofmonth_landslides = day_ofmonth_landslides.dropna()\n",
    "sns.displot(day_ofmonth_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052f78c",
   "metadata": {},
   "source": [
    "Yep, it looks like we did parse our dates correctly & this graph makes good sense to me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5075da",
   "metadata": {},
   "source": [
    "# CHARACTER ENCODING\n",
    "In this notebook, we're going to be working with different character encodings.\n",
    "\n",
    "Let's get started!\n",
    "The first thing we'll need to do is load in the libraries we'll be using. Not our dataset, though: we'll get to it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7a5efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import charset_normalizer\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e228b63",
   "metadata": {},
   "source": [
    "#### What are ENCODINGS?\n",
    "Character encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding than the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "Ã¦â€“â€¡Ã¥â€”Ã¥Å’â€“Ã£??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble.\n",
    "\n",
    "It was pretty hard to deal with encodings in Python 2, but thankfully in Python 3 it's a lot simpler. (Kaggle Notebooks only use Python 3.) There are two main data types you'll encounter when working with text in Python 3. One is is the string, which is what text is by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620d1278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = \"This is the euro symbol:  â‚¬\"\n",
    "type(before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731bd4a",
   "metadata": {},
   "source": [
    "The other data is the bytes data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it's in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d1647fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = before.encode('utf-8',errors='replace')\n",
    "type(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2be38",
   "metadata": {},
   "source": [
    "If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff07cb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This is the euro symbol:  \\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking a look at the bytes is in\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771987e",
   "metadata": {},
   "source": [
    "When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a651dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol:  â‚¬\n"
     ]
    }
   ],
   "source": [
    "print(after.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894cf75",
   "metadata": {},
   "source": [
    "However, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a CD player. If you try to play a cassette in a CD player, it just won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "663bff33",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 26: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_210635/4045691361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 26: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "print(after.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4368d",
   "metadata": {},
   "source": [
    "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ASCII using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e091f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the cedis symbol: ?\n"
     ]
    }
   ],
   "source": [
    "before = \"This is the cedis symbol: â‚µ\"\n",
    "after = before.encode(\"ascii\",errors='replace')\n",
    "print(after.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea2f72",
   "metadata": {},
   "source": [
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next.\n",
    "\n",
    "### Reading In Files With Encoding Problems\n",
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8762f6",
   "metadata": {},
   "source": [
    "#try to read in a file not in utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb0ecb1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x99 in position 7955: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_210635/3710767450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnot_utf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/kduah/Desktop/ML/03-Data-Cleaning/ksprojects.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1680\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x99 in position 7955: invalid start byte"
     ]
    }
   ],
   "source": [
    "not_utf = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/ksprojects.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3a17b",
   "metadata": {},
   "source": [
    "Notice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the charset_normalizer module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "I'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) Another reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2986531",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kduah/Desktop/ML/03-Data-Cleaning/ksprojects.csv','rb') as rawdata:\n",
    "    result = charset_normalizer.detect(rawdata.read(10000))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17052400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'windows-1250', 'language': 'English', 'confidence': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "427ec472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210635/4065738216.py:1: DtypeWarning: Columns (13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ksproject = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/ksprojects.csv',encoding='windows-1252')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09 11:36:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26 00:20:50</td>\n",
       "      <td>45000</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16 04:24:11</td>\n",
       "      <td>5000</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29 01:00:00</td>\n",
       "      <td>19500</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01 13:38:27</td>\n",
       "      <td>50000</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               name   \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000004038                                     Where is Hank?   \n",
       "2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "3  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "        category  main_category  currency             deadline   goal   \\\n",
       "0          Poetry     Publishing       GBP  2015-10-09 11:36:00   1000   \n",
       "1  Narrative Film   Film & Video       USD  2013-02-26 00:20:50  45000   \n",
       "2           Music          Music       USD  2012-04-16 04:24:11   5000   \n",
       "3    Film & Video   Film & Video       USD  2015-08-29 01:00:00  19500   \n",
       "4     Restaurants           Food       USD  2016-04-01 13:38:27  50000   \n",
       "\n",
       "             launched  pledged       state  backers  country  usd pledged   \\\n",
       "0  2015-08-11 12:12:28        0      failed        0       GB            0   \n",
       "1  2013-01-12 00:20:50      220      failed        3       US          220   \n",
       "2  2012-03-17 03:24:11        1      failed        1       US            1   \n",
       "3  2015-07-04 08:35:03     1283    canceled       14       US         1283   \n",
       "4  2016-02-26 13:38:27    52375  successful      224       US        52375   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16  \n",
       "0         NaN         NaN         NaN          NaN  \n",
       "1         NaN         NaN         NaN          NaN  \n",
       "2         NaN         NaN         NaN          NaN  \n",
       "3         NaN         NaN         NaN          NaN  \n",
       "4         NaN         NaN         NaN          NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksproject = pd.read_csv('/home/kduah/Desktop/ML/03-Data-Cleaning/ksprojects.csv',encoding='windows-1252')\n",
    "ksproject.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7db53",
   "metadata": {},
   "source": [
    "Yep, looks like charset_normalizer was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine.\n",
    "\n",
    "What if the encoding charset_normalizer guesses isn't right? Since charset_normalizer is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n",
    "\n",
    "\n",
    "#### Saving files with utf-8 encoding\n",
    "\n",
    "Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e47c8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksproject.to_csv('ksproject_converted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34e082ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalog.csv          ksproject_converted.csv  nfl-play-play.csv\r\n",
      "Data-Cleaning.ipynb  ksprojects.csv           README.md\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23dcd486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210635/1700460187.py:1: DtypeWarning: Columns (14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  attempting_read = pd.read_csv('./ksproject_converted.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09 11:36:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26 00:20:50</td>\n",
       "      <td>45000</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16 04:24:11</td>\n",
       "      <td>5000</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29 01:00:00</td>\n",
       "      <td>19500</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01 13:38:27</td>\n",
       "      <td>50000</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         ID                                               name   \\\n",
       "0           0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1           1  1000004038                                     Where is Hank?   \n",
       "2           2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "3           3  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "4           4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "        category  main_category  currency             deadline   goal   \\\n",
       "0          Poetry     Publishing       GBP  2015-10-09 11:36:00   1000   \n",
       "1  Narrative Film   Film & Video       USD  2013-02-26 00:20:50  45000   \n",
       "2           Music          Music       USD  2012-04-16 04:24:11   5000   \n",
       "3    Film & Video   Film & Video       USD  2015-08-29 01:00:00  19500   \n",
       "4     Restaurants           Food       USD  2016-04-01 13:38:27  50000   \n",
       "\n",
       "             launched  pledged       state  backers  country  usd pledged   \\\n",
       "0  2015-08-11 12:12:28        0      failed        0       GB            0   \n",
       "1  2013-01-12 00:20:50      220      failed        3       US          220   \n",
       "2  2012-03-17 03:24:11        1      failed        1       US            1   \n",
       "3  2015-07-04 08:35:03     1283    canceled       14       US         1283   \n",
       "4  2016-02-26 13:38:27    52375  successful      224       US        52375   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16  \n",
       "0         NaN         NaN         NaN          NaN  \n",
       "1         NaN         NaN         NaN          NaN  \n",
       "2         NaN         NaN         NaN          NaN  \n",
       "3         NaN         NaN         NaN          NaN  \n",
       "4         NaN         NaN         NaN          NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempting_read = pd.read_csv('./ksproject_converted.csv')\n",
    "attempting_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "370d32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Obtaining dependency information for python-Levenshtein from https://files.pythonhosted.org/packages/ae/9c/208f8ad7eb38492ac4f829790a500bcfca88b1d0a1c988f6480a52a6f681/python_Levenshtein-0.21.1-py3-none-any.whl.metadata\n",
      "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting Levenshtein==0.21.1 (from python-Levenshtein)\n",
      "  Obtaining dependency information for Levenshtein==0.21.1 from https://files.pythonhosted.org/packages/e6/02/0a4ed6a9e2b78f6b57f25a87fc194d7d10c2bbe95d985f36390e86285232/Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein)\n",
      "  Obtaining dependency information for rapidfuzz<4.0.0,>=2.3.0 from https://files.pythonhosted.org/packages/35/04/9ca97b17da457ed294519477da2aad0799c9ba8eebf37761a5ca94c35534/rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m152.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m281.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: gpg 1.16.0-unknown has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of gpg or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125ac2a",
   "metadata": {},
   "source": [
    "## Cleaning Out Inconsistent Data In Entries\n",
    "Efficiently fix typos in your data.\n",
    "In this notebook, we're going to learn how to clean up inconsistent text entries.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a65036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful modules\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import charset_normalizer\n",
    "\n",
    "professors = pd.read_csv('./pakistan_intel.csv')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b32703",
   "metadata": {},
   "source": [
    "Do some preliminary text pre-processingÂ¶\n",
    "We'll begin by taking a quick look at the first few rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab25dd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S#</th>\n",
       "      <th>Teacher Name</th>\n",
       "      <th>University Currently Teaching</th>\n",
       "      <th>Department</th>\n",
       "      <th>Province University Located</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Terminal Degree</th>\n",
       "      <th>Graduated from</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Area of Specialization/Research Interests</th>\n",
       "      <th>Other Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dr. Abdul Basit</td>\n",
       "      <td>University of Balochistan</td>\n",
       "      <td>Computer Science &amp; IT</td>\n",
       "      <td>Balochistan</td>\n",
       "      <td>Assistant Professor</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Asian Institute of Technology</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Engineering &amp; DBMS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Dr. Waheed Noor</td>\n",
       "      <td>University of Balochistan</td>\n",
       "      <td>Computer Science &amp; IT</td>\n",
       "      <td>Balochistan</td>\n",
       "      <td>Assistant Professor</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Asian Institute of Technology</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DBMS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Dr. Junaid Baber</td>\n",
       "      <td>University of Balochistan</td>\n",
       "      <td>Computer Science &amp; IT</td>\n",
       "      <td>Balochistan</td>\n",
       "      <td>Assistant Professor</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Asian Institute of Technology</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information processing, Multimedia mining</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Dr. Maheen Bakhtyar</td>\n",
       "      <td>University of Balochistan</td>\n",
       "      <td>Computer Science &amp; IT</td>\n",
       "      <td>Balochistan</td>\n",
       "      <td>Assistant Professor</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Asian Institute of Technology</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NLP, Information Retrieval, Question Answering...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>Samina Azim</td>\n",
       "      <td>Sardar Bahadur Khan Women's University</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Balochistan</td>\n",
       "      <td>Lecturer</td>\n",
       "      <td>BS</td>\n",
       "      <td>Balochistan University of Information Technolo...</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>VLSI Electronics DLD Database</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  S#         Teacher Name  \\\n",
       "0           2   3      Dr. Abdul Basit   \n",
       "1           4   5      Dr. Waheed Noor   \n",
       "2           5   6     Dr. Junaid Baber   \n",
       "3           6   7  Dr. Maheen Bakhtyar   \n",
       "4          24  25          Samina Azim   \n",
       "\n",
       "            University Currently Teaching             Department  \\\n",
       "0               University of Balochistan  Computer Science & IT   \n",
       "1               University of Balochistan  Computer Science & IT   \n",
       "2               University of Balochistan  Computer Science & IT   \n",
       "3               University of Balochistan  Computer Science & IT   \n",
       "4  Sardar Bahadur Khan Women's University       Computer Science   \n",
       "\n",
       "  Province University Located          Designation Terminal Degree  \\\n",
       "0                 Balochistan  Assistant Professor             PhD   \n",
       "1                 Balochistan  Assistant Professor             PhD   \n",
       "2                 Balochistan  Assistant Professor             PhD   \n",
       "3                 Balochistan  Assistant Professor             PhD   \n",
       "4                 Balochistan             Lecturer              BS   \n",
       "\n",
       "                                      Graduated from   Country    Year  \\\n",
       "0                      Asian Institute of Technology  Thailand     NaN   \n",
       "1                      Asian Institute of Technology  Thailand     NaN   \n",
       "2                      Asian Institute of Technology  Thailand     NaN   \n",
       "3                      Asian Institute of Technology  Thailand     NaN   \n",
       "4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n",
       "\n",
       "           Area of Specialization/Research Interests Other Information  \n",
       "0                        Software Engineering & DBMS               NaN  \n",
       "1                                               DBMS               NaN  \n",
       "2          Information processing, Multimedia mining               NaN  \n",
       "3  NLP, Information Retrieval, Question Answering...               NaN  \n",
       "4                      VLSI Electronics DLD Database               NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "professors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a200b",
   "metadata": {},
   "source": [
    "Say we're interested in cleaning up the \"Country\" column to make sure there's no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There's a more efficient way to do this, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "191a6983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n",
       "       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n",
       "       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n",
       "       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n",
       "       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n",
       "       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n",
       "       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all unique values in the country column\n",
    "countries = professors['Country'].unique()\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c52a69",
   "metadata": {},
   "source": [
    "Just looking at this, I can see some problems due to inconsistent data entry: ' Germany', and 'germany', for example, or ' New Zealand' and 'New Zealand'.\n",
    "\n",
    "The first thing I'm going to do is make everything lower case (I can change it back at the end if I like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bc08c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors['Country'] = professors['Country'].str.lower()\n",
    "professors['Country'] = professors['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abceac",
   "metadata": {},
   "source": [
    "Next we're going to tackle more difficult inconsistencies.\n",
    "\n",
    "### Use fuzzy matching to correct inconsistent data entryÂ¶\n",
    "Alright, let's take another look at the 'Country' column and see if there's any more data cleaning we need to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f31c0327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n",
       "       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n",
       "       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n",
       "       'norway', 'pakistan', 'portugal', 'russian federation',\n",
       "       'saudi arabia', 'scotland', 'singapore', 'south korea',\n",
       "       'southkorea', 'spain', 'sweden', 'thailand', 'turkey', 'uk',\n",
       "       'urbana', 'usa', 'usofa'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country =professors['Country'].unique()\n",
    "country.sort()\n",
    "country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56a237",
   "metadata": {},
   "source": [
    "It does look like there is another inconsistency: 'southkorea' and 'south korea' should be the same.\n",
    "\n",
    "We're going to use the fuzzywuzzy package to help identify which strings are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, itâ€™s fun!)\n",
    "\n",
    "Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "\n",
    "Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"south korea\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3642a68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South Korea', 100),\n",
       " ('SouthKorea', 48),\n",
       " ('Saudi Arabia', 43),\n",
       " ('Norway', 35),\n",
       " ('Ireland', 33),\n",
       " ('Portugal', 32),\n",
       " ('Singapore', 30),\n",
       " ('Netherland', 29),\n",
       " ('Macau', 25),\n",
       " ('USofA', 25)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5712339",
   "metadata": {},
   "source": [
    "We can see that two of the items in the cities are very close to \"south korea\": \"south korea\" and \"southkorea\". Let's replace all rows in our \"Country\" column that have a ratio of > 47 with \"south korea\".\n",
    "\n",
    "To do this, I'm going to write a function. (It's a good idea to write a general purpose function you can reuse if you think you might have to do a specific task more than once or twice. This keeps you from having to copy and paste code too often, which saves time and can help prevent mistakes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b978250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6987c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\n",
    "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac044f",
   "metadata": {},
   "source": [
    "Let us now take a closer look to see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71e4b409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n",
       "       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n",
       "       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n",
       "       'norway', 'pakistan', 'portugal', 'russian federation',\n",
       "       'saudi arabia', 'scotland', 'singapore', 'south korea', 'spain',\n",
       "       'sweden', 'thailand', 'turkey', 'uk', 'urbana', 'usa', 'usofa'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the unique values in the 'Country' column\n",
    "countries = professors['Country'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb82522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
