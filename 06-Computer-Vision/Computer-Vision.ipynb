{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97450c16",
   "metadata": {},
   "source": [
    "# COMPUTER VISION\n",
    "\n",
    "## Convolutional Classifier\n",
    "\n",
    "Create your first computer vision model with Keras\n",
    "Have you ever wanted to teach a computer to see? In this course, that's exactly what you'll do!\n",
    "\n",
    "In this course, you'll:\n",
    "\n",
    "Use modern deep-learning networks to build an image classifier with Keras\n",
    "Design your own custom convnet with reusable blocks\n",
    "Learn the fundamental ideas behind visual feature extraction\n",
    "Master the art of transfer learning to boost your models\n",
    "Utilize data augmentation to extend your dataset\n",
    "If you've taken the Introduction to Deep Learning course, you'll know everything you need to be successful.\n",
    "\n",
    "\n",
    "### Introduction\n",
    "This course will introduce you to the fundamental ideas of computer vision. Our goal is to learn how a neural network can \"understand\" a natural image well-enough to solve the same kinds of problems the human visual system can solve.\n",
    "\n",
    "The neural networks that are best at this task are called convolutional neural networks (Sometimes we say convnet or CNN instead.) Convolution is the mathematical operation that gives the layers of a convnet their unique structure. In future lessons, you'll learn why this structure is so effective at solving computer vision problems.\n",
    "\n",
    "We will apply these ideas to the problem of image classification: given a picture, can we train a computer to tell us what it's a picture of? You may have seen apps that can identify a species of plant from a photograph. That's an image classifier! In this course, you'll learn how to build image classifiers just as powerful as those used in professional applications.\n",
    "\n",
    "While our focus will be on image classification, what you'll learn in this course is relevant to every kind of computer vision problem. At the end, you'll be ready to move on to more advanced applications like generative adversarial networks and image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47dfec",
   "metadata": {},
   "source": [
    "The Convolutional Classifier\n",
    "A convnet used for image classification consists of two parts: \n",
    "a) convolutional base and\n",
    "a) dense head.\n",
    "This is visually represented below:\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/U0n5xjU.png\n",
    "\n",
    "The base is used to extract the features from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well. (You'll learn about these in the next lesson.)\n",
    "\n",
    "The head is used to determine the class of the image. It is formed primarily of dense layers, but might include other layers like dropout.\n",
    "\n",
    "What do we mean by visual feature? A feature could be a line, a color, a texture, a shape, a pattern -- or some complicated combination.\n",
    "\n",
    "The whole process goes something like this:\n",
    "\n",
    "Visual representation in the image in the image below:\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/UUAafkn.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee28c9",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "The goal of the network during training is to learn two things:\n",
    "\n",
    "which features to extract from an image (base),\n",
    "which class goes with what features (head).\n",
    "These days, convnets are rarely trained from scratch. More often, we reuse the base of a pretrained model. To the pretrained base we then attach an untrained head. In other words, we reuse the part of a network that has already learned to do 1. Extract features, and attach to it some fresh layers to learn 2. Classify.\n",
    "\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/E49fsmV.png\n",
    "\n",
    "Because the head usually consists of only a few dense layers, very accurate classifiers can be created from relatively little data.\n",
    "\n",
    "Reusing a pretrained model is a technique known as transfer learning. It is so effective, that almost every image classifier these days will make use of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82810330",
   "metadata": {},
   "source": [
    "Example - Train a Convnet Classifier\n",
    "Throughout this course, we're going to be creating classifiers that attempt to solve the following problem: is this a picture of a Car or of a Truck? Our dataset is about 10,000 pictures of various automobiles, around half cars and half trucks.\n",
    "\n",
    "Step 1 - Load Data\n",
    "This next hidden cell will import some libraries and set up our data pipeline. We have a training split called ds_train and a validation split called ds_valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddb764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 files belonging to 2 classes.\n",
      "Found 5051 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Reproducability\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed(31415)\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "# Load training and validation sets\n",
    "ds_train_ = image_dataset_from_directory(\n",
    "    './train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_valid_ = image_dataset_from_directory(\n",
    "    './valid',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e9659",
   "metadata": {},
   "source": [
    "Step 2 - Define Pretrained BaseÂ¶\n",
    "The most commonly used dataset for pretraining is ImageNet, a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its applications module. The pretrained model we'll use is called VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51732111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "pretrained_base = tf.keras.models.load_model(\n",
    "    './vgg16-pretrained-base',\n",
    ")\n",
    "pretrained_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d643d68",
   "metadata": {},
   "source": [
    "Step 3 - Attach Head\n",
    "Next, we attach the classifier head. For this example, we'll use a layer of hidden units (the first Dense layer) followed by a layer to transform the outputs to a probability score for class 1, Truck. The Flatten layer transforms the two dimensional outputs of the base into the one dimensional inputs needed by the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8a3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    pretrained_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeba8be",
   "metadata": {},
   "source": [
    "Step 4 - Train\n",
    "Finally, let's train the model. Since this is a two-class problem, we'll use the binary versions of crossentropy and accuracy. The adam optimizer generally performs well, so we'll choose it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4d335",
   "metadata": {},
   "source": [
    "When training a neural network, it's always a good idea to examine the loss and metric plots. The history object contains this information in a dictionary history.history. We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9575c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80665868",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the structure of a convnet classifier: a head to act as a classifier atop of a base which performs the feature extraction.\n",
    "\n",
    "The head, essentially, is an ordinary classifier like you learned about in the introductory course. For features, it uses those features extracted by the base. This is the basic idea behind convolutional classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n",
    "\n",
    "This is one of the big advantages deep neural networks have over traditional machine learning models: given the right network structure, the deep neural net can learn how to engineer the features it needs to solve its problem.\n",
    "\n",
    "For the next few lessons, we'll take a look at how the convolutional base accomplishes the feature extraction. Then, you'll learn how to apply these ideas and design some classifiers of your own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a215e9",
   "metadata": {},
   "source": [
    "# CONVOLUTION AND RELU\n",
    "\n",
    "Discover how convnets create features with convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af10c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def show_kernel(kernel, label=True, digits=None, text_size=28):\n",
    "    # Format kernel\n",
    "    kernel = np.array(kernel)\n",
    "    if digits is not None:\n",
    "        kernel = kernel.round(digits)\n",
    "\n",
    "    # Plot kernel\n",
    "    cmap = plt.get_cmap('Blues_r')\n",
    "    plt.imshow(kernel, cmap=cmap)\n",
    "    rows, cols = kernel.shape\n",
    "    thresh = (kernel.max()+kernel.min())/2\n",
    "    # Optionally, add value labels\n",
    "    if label:\n",
    "        for i, j in product(range(rows), range(cols)):\n",
    "            val = kernel[i, j]\n",
    "            color = cmap(0) if val > thresh else cmap(255)\n",
    "            plt.text(j, i, val, \n",
    "                     color=color, size=text_size,\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ebfa0",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that a convolutional classifier has two parts: a convolutional base and a head of dense layers. We learned that the job of the base is to extract visual features from an image, which the head would then use to classify the image.\n",
    "\n",
    "Over the next few lessons, we're going to learn about the two most important types of layers that you'll usually find in the base of a convolutional image classifier. These are the convolutional layer with ReLU activation, and the maximum pooling layer. In Lesson 5, you'll learn how to design your own convnet by composing these layers into blocks that perform the feature extraction.\n",
    "\n",
    "This lesson is about the convolutional layer with its ReLU activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a728f26",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Before we get into the details of convolution, let's discuss the purpose of these layers in the network. We're going to see how these three operations (convolution, ReLU, and maximum pooling) are used to implement the feature extraction process.\n",
    "\n",
    "The feature extraction performed by the base consists of three basic operations:\n",
    "\n",
    "Filter an image for a particular feature (convolution)\n",
    "Detect that feature within the filtered image (ReLU)\n",
    "Condense the image to enhance the features (maximum pooling)\n",
    "The next figure illustrates this process. You can see how these three operations are able to isolate some particular characteristic of the original image (in this case, horizontal lines).\n",
    "Visually represented in the image from the link below:\n",
    "\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/IYO9lqp.png\n",
    "\n",
    "Typically, the network will perform several extractions in parallel on a single image. In modern convnets, it's not uncommon for the final layer in the base to be producing over 1000 unique visual features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9ecaa",
   "metadata": {},
   "source": [
    "Filter with Convolution\n",
    "A convolutional layer carries out the filtering step. You might define a convolutional layer in a Keras model something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a746fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64,kernel_size=3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab01e8",
   "metadata": {},
   "source": [
    "We can understand these parameters by looking at their relationship to the weights and activations of the layer. Let's do that now.\n",
    "\n",
    "Weights\n",
    "The weights a convnet learns during training are primarily contained in its convolutional layers. These weights we call kernels. We can represent them as small arrays:\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/uJfD9r9.png\n",
    "\n",
    "A kernel operates by scanning over an image and producing a weighted sum of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/j3lk26U.png\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb3508",
   "metadata": {},
   "source": [
    "Kernels define how a convolutional layer is connected to the layer that follows. The kernel above will connect each neuron in the output to nine neurons in the input. By setting the dimensions of the kernels with kernel_size, you are telling the convnet how to form these connections. Most often, a kernel will have odd-numbered dimensions -- like kernel_size=(3, 3) or (5, 5) -- so that a single pixel sits at the center, but this is not a requirement.\n",
    "\n",
    "The kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. This means finding the best values for its kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fa6f9",
   "metadata": {},
   "source": [
    "Activations\n",
    "The activations in the network we call feature maps. They are what result when we apply a filter to an image; they contain the visual features the kernel extracts. Here are a few kernels pictured with feature maps they produced.\n",
    "\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/JxBwchH.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c40d9f",
   "metadata": {},
   "source": [
    "From the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. Generally, what a convolution accentuates in its inputs will match the shape of the positive numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes.\n",
    "\n",
    "With the filters parameter, you tell the convolutional layer how many feature maps you want it to create as output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95481a3f",
   "metadata": {},
   "source": [
    "Detect with ReLU\n",
    "After filtering, the feature maps pass through the activation function. The rectifier function has a graph like this:\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/DxGJuTH.png\n",
    "\n",
    "The graph of the rectifier function looks like a line with the negative part \"rectified\" to 0.\n",
    "A neuron with a rectifier attached is called a rectified linear unit. For that reason, we might also call the rectifier function the ReLU activation or even the ReLU function.\n",
    "\n",
    "The ReLU activation can be defined in its own Activation layer, but most often you'll just include it as the activation function of Conv2D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9657e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu')\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d3432",
   "metadata": {},
   "source": [
    "You could think about the activation function as scoring pixel values according to some measure of importance. The ReLU activation says that negative values are not important and so sets them to 0. (\"Everything unimportant is equally unimportant.\")\n",
    "\n",
    "Here is ReLU applied the feature maps above. Notice how it succeeds at isolating the features.\n",
    "Notice how the activation performed in the image in the link below:\n",
    "https://storage.googleapis.com/kaggle-media/learn/images/dKtwzPY.png\n",
    "\n",
    "Like other activation functions, the ReLU function is nonlinear. Essentially this means that the total effect of all the layers in the network becomes different than what you would get by just adding the effects together -- which would be the same as what you could achieve with only a single layer. The nonlinearity ensures features will combine in interesting ways as they move deeper into the network. (We'll explore this \"feature compounding\" more in Lesson 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbab24",
   "metadata": {},
   "source": [
    "Example - Apply Convolution and ReLU\n",
    "We'll do the extraction ourselves in this example to understand better what convolutional networks are doing \"behind the scenes\".\n",
    "\n",
    "Here is the image we'll use for this example:\n",
    "\n",
    "https://www.kaggleusercontent.com/kf/126572661/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..q2DGW4_UEXi1L9le1jr5lw.jtXIyIJC2QQHebD1vJm5bFUv7bl9G1iEy-z_Gyh6oQwRC3iFk6tbF-iQ7IpY3dYsciIp-5ZLA8YiFKWPBw2cHCG_bmvEVZ32HrSoCsbkeMnJUncFPh8tZ2T1eCtqzNdUwqhw0RktSHjisI-fNMCge3byh5OkSy9fdUWqt8DxNndl8LyP3TFXXs0C-swzLwX_ISrDdAB4v9NOUUTQZvePzTik2csjDPXOpgTJIIFpPUKExVMre2jL2wcgUzNZqKh_mo5pVSIjVqJiCjHhDjf7uAxjlqGuv8goTjMr5N2obtW0yA-nr-LzvKTyFxDq9kwl8Iz_T5u2jzmSdkmhtogyq161kGSVZbjWnSssHhUldie7YGg5B5Cra_gHf0iFqX07pdbtQkoNEIi4-ogYR9PLrqvYM1Xd5HeUl_6-NOhnDTnu0vk6ZZ6aaAB27pYt8AjhOjq4qGczgorkYiQCNofW1dwxK4np081QRIizFSMizyDQ7BarYkD24Hixd24_SFePZkmRNoSc-I5r14usGGMlHnCEktZSO0Ct0tHgxkgeCD3fO77wRDA4KGwFPZ1GXbJ6pNSewzhpsZZj5Dz4u4JgaxxmDQ-m9b7TkwTqHKbyEQF5gSSgMGsMWsfCpoDuzk8r2EIOzdYdKNx7LQt1aGii_0_7UNkTK-yS9mx2yACwbAg.BmxBRIAWJ-EX7Sr9nYd9tQ/__results___files/__results___7_0.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f571688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "image_path = './car_feature.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.io.decode_jpeg(image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
